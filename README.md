# Multimodal Deep Learning

## Papers
[Cross-Modal BERT for Text-Audio Sentiment Analysis](https://github.com/thuiar/Cross-Modal-BERT), MM 2020

[Memory fusion network for multi-view sequential learning](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17341/16122), AAAI 2018

[Tensor fusion network for multimodal sentiment analysis(TFN)](https://www.aclweb.org/anthology/D17-1115.pdf), ACL 2017

[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://arxiv.org/pdf/1906.00295.pdf), ACL 2019, [Github](https://github.com/yaohungt/Multimodal-Transformer)

[Adapting BERT for Target-Oriented Multimodal Sentiment Classification](https://www.ijcai.org/Proceedings/2019/0751.pdf), IJCAI 2019

[Integrating Multimodal Information in Large Pretrained Transformers](https://arxiv.org/abs/1908.05787), ACL 2020

[M-bert: Injecting multimodal information in the bert structure](https://arxiv.org/abs/1908.05787), arXiv 2019

[Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation](https://arxiv.org/abs/2005.08575), arXiv 2020

[Multimodal big data affective analytics: A comprehensive survey using text,audio, visual and physiological signals](https://www.sciencedirect.com/science/article/pii/S1084804519303078)


